{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briceshun/Documents/Personal Projects/rag-test/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/briceshun/Documents/Personal Projects/rag-test/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/briceshun/Documents/Personal Projects/rag-test/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/briceshun/Documents/Personal Projects/rag-test/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/briceshun/Documents/Personal Projects/rag-test/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "# from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "# from llama_index.core.agent import ReActAgent\n",
    "\n",
    "# Load model\n",
    "from agents import load_model\n",
    "model, pipe = load_model('agents/Llama-3_2-3B-Instruct')\n",
    "\n",
    "# Load retriever\n",
    "from agents import llamaindex_retriever\n",
    "\n",
    "# Load prompts\n",
    "from agents import prompts\n",
    "system_prompt = prompts.metadata_prompt\n",
    "sql_prompt = prompts.sql_prompt\n",
    "checker_prompt = prompts.checker_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create database replica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples and Info Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables\n",
    "l_tables = db.get_usable_table_names()\n",
    "\n",
    "# Create metadata\n",
    "d_metadata = {}\n",
    "for table in l_tables:\n",
    "    # Sample 10 rows from each table\n",
    "    sample = db.run(f\"SELECT * FROM {table} LIMIT 5;\", fetch=\"cursor\")\n",
    "    # Get table schema\n",
    "    infoschema = db.run(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table}';\", fetch=\"cursor\")\n",
    "    # Store metadata\n",
    "    d_metadata[table] = {\n",
    "        \"sample\": list(sample.mappings()),\n",
    "        \"infoschema\": list(infoschema.mappings())[0]['sql'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Album\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Artist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Customer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Employee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Genre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Invoice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: InvoiceLine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: MediaType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Playlist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing table Playlist: Expecting ',' delimiter: line 3 column 207 (char 513)\n",
      "Retrying... (1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: PlaylistTrack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for table in l_tables:\n",
    "    print(f\"Processing table: {table}\")\n",
    "    # Generate response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{d_metadata[table]}\"},\n",
    "    ]\n",
    "    response = pipe(messages)\n",
    "\n",
    "    # Clean response\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = pipe(messages)\n",
    "            json_string = response[0]['generated_text'][2]['content'].replace(\"'\", '\"')\n",
    "            data_dict = json.loads(json_string)\n",
    "            break  # Exit loop if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table {table}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise  # Re-raise the exception if max retries reached\n",
    "            print(f\"Retrying... ({attempt + 1}/{max_retries})\")\n",
    "\n",
    "    # Store metadata\n",
    "    d_metadata[table]['description'] = data_dict['description']\n",
    "    d_metadata[table]['use case'] = data_dict['use case']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RowMapping to Dict\n",
    "for table in l_tables:\n",
    "    d_metadata[table]['sample'] = [dict(row) for row in d_metadata[table]['sample']]\n",
    "\n",
    "# Save dictionary to a JSON file\n",
    "with open('data/metadata.json', 'w') as json_file:\n",
    "    json.dump(d_metadata, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/metadata.json', 'r') as json_file:\n",
    "    d_metadata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text files\n",
    "for table in l_tables:\n",
    "    with open(f'data/metadata/{table}.txt', 'w') as txt_file:\n",
    "        txt_file.write(f\"\\n\\nTable: {table}\\n\\n\")\n",
    "        txt_file.write(f\"Description: \\n{d_metadata[table]['description']}\\n\\n\")\n",
    "        txt_file.write(f\"Use Case: \\n{d_metadata[table]['use case']}\\n\\n\")\n",
    "        txt_file.write(f\"Schema: \\n{d_metadata[table]['infoschema']}\\n\\n\")\n",
    "        txt_file.write(f\"Sample:\\n\")\n",
    "        for row in d_metadata[table]['sample']:\n",
    "            txt_file.write(f\"{row}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_engine = llamaindex_retriever(\n",
    "    [f'data/metadata/{table}.txt' for table in l_tables]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT T1.Title, COUNT(T2.TrackId) AS NumTracks \n",
      "FROM Album AS T1 \n",
      "JOIN Track AS T2 ON T1.AlbumId = T2.AlbumId \n",
      "GROUP BY T1.Title \n",
      "ORDER BY NumTracks DESC LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Which album has the most tracks?\"\n",
    "tables = db_engine.retrieve(prompt)\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": sql_prompt},\n",
    "        {\"role\": \"sqlagent\", \"content\": '\\n'.join([t.text for t in tables])},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "response = pipe(messages)\n",
    "query = response[0]['generated_text'][3]['content'].replace('```sql', '').replace('```', '').strip()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[('Greatest Hits', 57)]\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(fr\"\"\"{query}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT SUM(T2.Total) FROM Customer AS T1 INNER JOIN Invoice AS T2 ON T1.CustomerId = T2.CustomerId WHERE T1.Country = 'Brazil'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What were the total sales from customers in Brazil?\"\n",
    "tables = db_engine.retrieve(prompt)\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": sql_prompt},\n",
    "        {\"role\": \"sqlagent\", \"content\": '\\n'.join([t.text for t in tables])},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "response = pipe(messages)\n",
    "query = response[0]['generated_text'][3]['content'].replace('```sql', '').replace('```', '').strip()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(190.1,)]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(fr\"\"\"{query}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT T1.Name, SUM(T2.Total) AS TotalSales \n",
      "FROM Track AS T1 \n",
      "INNER JOIN InvoiceLine AS T3 ON T1.TrackId = T3.TrackId \n",
      "INNER JOIN Invoice AS T2 ON T3.InvoiceId = T2.InvoiceId \n",
      "INNER JOIN Customer AS T4 ON T2.CustomerId = T4.CustomerId \n",
      "WHERE T4.Country = 'Brazil' \n",
      "GROUP BY T1.Name \n",
      "ORDER BY TotalSales DESC \n",
      "LIMIT 10\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What were the top 10 total sales in $ by track from customers in Brazil?\"\n",
    "tables = db_engine.retrieve(prompt)\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": sql_prompt},\n",
    "        {\"role\": \"sqlagent\", \"content\": '\\n'.join([t.text for t in tables])},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "response = pipe(messages)\n",
    "query = response[0]['generated_text'][3]['content'].replace('```sql', '').replace('```', '').strip()\n",
    "print(query)\n",
    "try:\n",
    "    db.run(fr\"\"\"{query}\"\"\")\n",
    "except Exception as e:\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": checker_prompt},\n",
    "            {\"role\": \"sqlagent\", \"content\": '\\n'.join([t.text for t in tables])},\n",
    "            {\"role\": \"user\", \"content\": e},\n",
    "        ]\n",
    "    response = pipe(messages)\n",
    "    query = response[0]['generated_text'][3]['content'].replace('```sql', '').replace('```', '').strip()\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(\\'Untitled\\', 27.72), (\\'Água de Beber\\', 13.86), (\\'Your Blue Room\\', 13.86), (\"You\\'ve Been A Long Time Coming\", 13.86), (\"You\\'re My Best Friend\", 13.86), (\\'X-9 2001\\', 13.86), (\\'Why Go\\', 13.86), (\\'Wanted Dread And Alive\\', 13.86), (\\'Vai Valer\\', 13.86), (\\'TriboTchan\\', 13.86)]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(fr\"\"\"{query}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
